# Plan: LLM-Based Image Diversity Assessment

## Problem

The current workflow for assessing whether a Wikipedia career article or its
linked Commons category has diverse imagery is entirely manual. A human reviewer
opens each career, looks at the images, and selects a status (`needs_diverse_images`,
`has_diverse_images`, etc.). With ~4,000 careers in the dataset, this is slow and
subjective. A multimodal LLM can automate or assist this assessment by analyzing
the actual image content alongside captions and career context.

## Approach

Use a vision-capable LLM (Claude, GPT-4o, or Gemini) to analyze sets of images
associated with a career and produce a structured diversity assessment. The model
receives:

1. The career name and article lede (context about what the career is)
2. The current images (as URLs or base64) with their captions/descriptions
3. A prompt asking it to assess representation across multiple diversity dimensions

The model returns a structured JSON response with per-dimension assessments and
an overall recommendation that maps to the existing review statuses.

## Inputs Per Assessment

### For Wikipedia Article Images

- **Career name** (e.g., "Architect")
- **Article lede** (first ~800 chars of the article, already stored in DB)
- **Images**: up to 20 article images, each with:
  - Thumbnail URL (400px, already generated by Wikipedia API)
  - Caption/description from `ImageDescription` metadata
- **Main thumbnail URL** (the lead image)

### For Commons Category Images

- **Career name**
- **Commons category name** (e.g., "Category:Architects")
- **Images**: a sample of category files (e.g., 20-50), each with:
  - Thumbnail URL (300px, already generated by Commons API)
  - Description from `ImageDescription` metadata

## Prompt Design

The prompt should be carefully structured to:

1. **Define diversity dimensions** the model should assess:
   - Gender presentation
   - Apparent racial/ethnic diversity
   - Age range
   - Disability representation (where visible)
   - Geographic/cultural context (clothing, settings, backgrounds)

2. **Provide career context** so the model understands what's being depicted
   and can distinguish between images of people practicing the career vs.
   tools/locations/logos that happen to be in the article.

3. **Request structured output** as JSON for reliable parsing. Example schema:

```json
{
  "images_analyzed": 8,
  "images_depicting_people": 5,
  "assessment": {
    "gender": {
      "observed": ["male"],
      "diversity_level": "low",
      "notes": "All 5 images showing people depict men"
    },
    "ethnicity": {
      "diversity_level": "low",
      "notes": "All people depicted appear to be of European descent"
    },
    "age": {
      "diversity_level": "medium",
      "notes": "Mix of middle-aged and older individuals"
    },
    "geographic_context": {
      "diversity_level": "low",
      "notes": "All settings appear to be Western/European"
    }
  },
  "overall_diversity": "low",
  "recommended_status": "needs_diverse_images",
  "suggested_search_terms": [
    "female architect",
    "Black architect",
    "architect Asia"
  ],
  "reasoning": "The article images exclusively show white men. The career of architect is practiced globally by people of all genders and backgrounds, so these images do not represent the diversity of the profession."
}
```

4. **Handle edge cases** in the prompt:
   - Articles with no images of people (diagrams, tools, buildings only)
     -> map to `no_picture` or a new `no_people_in_images` status
   - Gender-specific careers (e.g., "abbess") -> detect and flag
   - Articles that aren't really careers -> detect and flag
   - Low-quality or ambiguous images -> note uncertainty

### Sample Prompt Template

```
You are assessing the diversity of images used in a Wikipedia article about a
career/profession. Your goal is to determine whether the images represent the
diversity of people who work in this career.

Career: {career_name}
Article context: {lede_text}

I will show you {n} images from this article with their captions.

For each image, determine:
1. Does it depict a person or people practicing this career?
2. If so, what can you observe about the person's apparent demographics?

Then provide an overall diversity assessment as JSON with this schema:
{schema}

Important guidelines:
- Only assess images that actually depict people. Logos, diagrams, tools,
  buildings, etc. should be noted but not factored into diversity scoring.
- Use "low", "medium", or "high" for diversity levels.
- Be respectful and use observational language ("appears to be", "presents as").
- If the career is inherently gender-specific (e.g., "nun", "abbess"), note this.
- If fewer than 2 images depict people, set recommended_status to "no_picture".
- Suggest specific search terms that would help find underrepresented groups.
```

## Architecture

### New Module: `llm_assess.py`

A new module responsible for all LLM-based assessment logic:

```python
# llm_assess.py

def assess_career_images(career: dict, images: list[dict]) -> dict:
    """Assess diversity of a career's Wikipedia article images.

    Args:
        career: dict with keys: name, lede_text, wikidata_id
        images: list of dicts with keys: thumb_url, caption

    Returns:
        Structured assessment dict (see schema above)
    """

def assess_commons_category(career: dict, category: str,
                            images: list[dict]) -> dict:
    """Assess diversity of a Commons category's images.

    Args:
        career: dict with keys: name, wikidata_id
        category: Commons category name
        images: list of dicts with keys: thumb_url, description

    Returns:
        Structured assessment dict (see schema above)
    """

def build_prompt(career: dict, images: list[dict],
                 assessment_type: str) -> list:
    """Build the multimodal prompt (text + image references)."""

def parse_assessment(response: str) -> dict:
    """Parse and validate the LLM's JSON response."""
```

### LLM API Integration

Support multiple providers behind a common interface:

```python
# Provider configuration via environment variables
LLM_PROVIDER = os.environ.get("LLM_PROVIDER", "anthropic")  # or "openai", "google"
LLM_MODEL = os.environ.get("LLM_MODEL", "claude-sonnet-4-20250514")
LLM_API_KEY = os.environ.get("LLM_API_KEY")
```

**Image delivery strategy:**

- **URL-based** (preferred for Claude/Gemini): Pass thumbnail URLs directly.
  Wikipedia and Commons thumbnails are publicly accessible and stable. This
  avoids downloading and base64-encoding images.
- **Base64 fallback**: Download thumbnails and encode as base64 if the provider
  doesn't support URL-based image input or if URLs are unreliable.

Use 300-400px thumbnails (already available from existing APIs) to keep token
costs reasonable while providing enough detail for demographic assessment.

### Database Changes

Add columns to store LLM assessment results:

```sql
ALTER TABLE careers ADD COLUMN llm_assessment TEXT;          -- JSON blob
ALTER TABLE careers ADD COLUMN llm_assessed_at TEXT;         -- timestamp
ALTER TABLE careers ADD COLUMN llm_recommended_status TEXT;  -- the LLM's recommendation
ALTER TABLE careers ADD COLUMN llm_commons_assessment TEXT;  -- JSON blob for Commons
ALTER TABLE careers ADD COLUMN llm_commons_assessed_at TEXT; -- timestamp
```

Storing the full JSON assessment allows the UI to display detailed breakdowns
without re-running the LLM. The `llm_recommended_status` column enables
filtering/sorting by LLM recommendation independently of the human review status.

### Integration Points

#### 1. Batch Assessment CLI (`fetcher.py` or new command)

```bash
uv run python fetcher.py assess              # Assess all unreviewed careers
uv run python fetcher.py assess --limit 50   # Assess 50 careers
uv run python fetcher.py assess --reassess   # Re-assess previously assessed
uv run python fetcher.py assess-commons      # Assess Commons categories
```

This runs assessments in batch, fetching images (via existing `wikipedia.py` /
`commons.py` functions) and calling the LLM for each career. Results are stored
in the database.

**Rate limiting and cost control:**

- Process careers sequentially (LLM APIs have rate limits)
- Log cost estimates per assessment (based on image count and token usage)
- Support `--dry-run` to estimate costs without calling the API
- Skip careers that already have `llm_assessed_at` set (unless `--reassess`)

#### 2. On-Demand Assessment in Web UI

Add a button to the career detail page: **"Get LLM Assessment"**

- Calls a new Flask endpoint: `POST /career/<wikidata_id>/assess`
- Runs the LLM assessment on the current images
- Returns the structured result and displays it in the UI
- Shows the per-dimension breakdown, overall recommendation, and suggested
  search terms
- The reviewer can use this as input to their manual review decision

#### 3. Assessment Display in UI

On the career detail page, show the LLM assessment (if available) as a panel:

- Overall diversity rating (low/medium/high) with color coding
- Per-dimension breakdown (gender, ethnicity, age, geographic context)
- Number of images analyzed vs. number depicting people
- Suggested search terms (clickable, pre-populate the Openverse search)
- LLM's reasoning text
- Timestamp of when assessment was run
- Button to re-run assessment if images have changed

On the career list page, add:

- A column or badge showing LLM-recommended status
- Filter option: "Show careers where LLM recommends X"
- Sort option: "LLM diversity score"

#### 4. Prioritization Aid

Use LLM assessments to help prioritize human review:

- Sort careers by: high pageviews + low LLM diversity score = highest priority
- Flag disagreements between LLM recommendation and human review status
- Dashboard showing assessment distribution (how many low/medium/high)

## Image Sampling Strategy

### Wikipedia Articles

- Use all available article images (up to 20, already fetched by `wikipedia.py`)
- Always include the main thumbnail
- Filter out non-photographic images (SVGs, diagrams) before sending to LLM
  to reduce noise and cost. The existing `_is_icon_or_template` filter in
  `wikipedia.py` already removes many of these.

### Commons Categories

- Categories can have hundreds or thousands of files
- Sample strategy: fetch the first 30-50 images (sorted by most recent upload)
- For large categories, consider sampling from multiple pages to avoid recency
  bias
- Skip SVG files and files with known non-photographic extensions (.svg, .ogg,
  .pdf, .djvu)

## Model Selection Considerations

| Consideration     | Claude (Anthropic)  | GPT-4o (OpenAI)    | Gemini (Google)     |
|-------------------|---------------------|---------------------|---------------------|
| Vision quality    | Strong              | Strong              | Strong              |
| Image input       | URL or base64       | URL or base64       | URL or base64       |
| Max images/request| ~20                 | ~20                 | Large context       |
| Structured output | JSON mode           | JSON mode           | JSON mode           |
| Cost per image    | Moderate            | Moderate            | Lower               |
| Rate limits       | Moderate            | Moderate            | Generous            |

**Recommendation:** Start with Claude (claude-sonnet-4-20250514) for best balance of
quality and cost. The project is already Python-based, and the Anthropic SDK
has good multimodal support. Make the provider configurable so it's easy to
switch or compare.

## Cost Estimation

Rough per-career cost (assuming 10 images at 400px thumbnails):

- ~10 images x ~1,000 tokens each = ~10,000 image tokens
- ~500 tokens for prompt text
- ~500 tokens for response
- Total: ~11,000 tokens per assessment
- At Claude Sonnet pricing (~$3/M input, $15/M output):
  ~$0.03 input + ~$0.008 output = **~$0.04 per career**
- For 4,000 careers: **~$160 total** for a full pass

This is very manageable. Commons category assessments would be similar per
category, with ~43% of careers having a linked category (~1,700 assessments).

## Error Handling

- **LLM returns invalid JSON**: Retry once with a more explicit formatting
  instruction. If still invalid, log the raw response and mark as failed.
- **LLM refuses to assess** (content policy): Log and skip. Some images may
  trigger content filters. Store the refusal so it isn't retried endlessly.
- **Image URL 404s**: Skip that image, note it in the assessment. Proceed with
  remaining images.
- **Rate limit hit**: Exponential backoff with jitter. Log progress so batch
  runs can be resumed.
- **Fewer than 2 images of people**: The LLM should flag this in its response.
  Map to appropriate status.

## Ethical Considerations

- **No identity classification**: The LLM should describe what it observes in
  images (apparent presentation) rather than asserting identity. Prompt language
  should use "appears to present as" rather than "is".
- **Transparency**: Make it clear in the UI that assessments are LLM-generated,
  not human judgments. Always show the LLM's reasoning.
- **Human final say**: LLM assessments are advisory. The human reviewer's
  status selection remains authoritative. The `status` column continues to
  reflect human decisions; `llm_recommended_status` is separate.
- **Bias awareness**: LLMs may have biases in demographic perception. Periodic
  spot-checks of LLM assessments against human reviews can identify systematic
  issues.
- **Privacy**: Only publicly-available Wikipedia/Commons images are analyzed.
  No facial recognition or identification is performed.

## Implementation Phases

### Phase 1: Core Assessment Module

- [ ] Create `llm_assess.py` with prompt building, API calling, response parsing
- [ ] Add Anthropic SDK dependency (`anthropic` package)
- [ ] Write prompt template with structured JSON output schema
- [ ] Add basic unit tests with mocked LLM responses
- [ ] Add `LLM_API_KEY` and `LLM_PROVIDER` environment variable support

### Phase 2: Batch CLI

- [ ] Add `assess` subcommand to `fetcher.py`
- [ ] Database migration for `llm_assessment`, `llm_assessed_at`, `llm_recommended_status` columns
- [ ] Implement rate limiting, progress logging, resume capability
- [ ] Add `--dry-run` mode for cost estimation
- [ ] Run initial batch on a small sample, validate results

### Phase 3: Web UI Integration

- [ ] Add "Get LLM Assessment" button to career detail page
- [ ] Create `/career/<id>/assess` endpoint
- [ ] Display assessment panel with per-dimension breakdown
- [ ] Make suggested search terms clickable (pre-populate Openverse search)
- [ ] Add LLM recommendation column/badge to career list page
- [ ] Add filter/sort by LLM recommendation

### Phase 4: Commons Category Assessment

- [ ] Extend `llm_assess.py` with Commons-specific prompt and sampling
- [ ] Add `assess-commons` subcommand
- [ ] Database migration for `llm_commons_assessment`, `llm_commons_assessed_at`
- [ ] Display Commons assessment in the Commons review UI

### Phase 5: Prioritization and Analytics

- [ ] Build priority score: pageviews x inverse-diversity-score
- [ ] Dashboard showing assessment distribution across all careers
- [ ] Flag human/LLM disagreements for re-review
- [ ] Track assessment accuracy over time (compare LLM vs. human decisions)
